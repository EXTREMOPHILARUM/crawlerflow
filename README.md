# Web-Crawler

A micro-framework to crawl the web pages with crawlers configs. 
It can use [MongoDB](https://www.mongodb.com/), [Elasticsearch](https://www.elastic.co/products/elasticsearch) 
and [Solr](http://lucene.apache.org/solr/) databases to cache and save the extracted data. 


## Install

```bash

pip install web-crawler-plus

```

Checkout the [documentation](docs/index.md) for more information.


